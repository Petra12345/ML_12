{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_selection, linear_model, metrics, model_selection, tree, ensemble\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn import over_sampling\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(df.dtypes)\n",
    "# pd.set_option('display.max_rows', 10)-\n",
    "\n",
    "\n",
    "def remove_columns_missing_values(df):\n",
    "    # Missing values percentages percentage\n",
    "    count_missing_per = df.isnull().sum()\n",
    "    count_missing_per.describe()\n",
    "    count_missing_per = count_missing_per.div(len(df))\n",
    "\n",
    "    # Test if NaN\n",
    "    # Cut-off\n",
    "    missing_cut_off = 0.6\n",
    "    missing_data_cut_off = count_missing_per[count_missing_per < missing_cut_off]\n",
    "    labels_included_variables = missing_data_cut_off.keys()\n",
    "    data_without_columns_missing = df[labels_included_variables]\n",
    "    return data_without_columns_missing\n",
    "\n",
    "\n",
    "def remove_rows_with_missing_values(df):\n",
    "    return df[df.notnull().all(axis=1)]\n",
    "\n",
    "\n",
    "def apply_MICE(df):\n",
    "    start = time.time()\n",
    "    labels = df.columns.tolist()\n",
    "    imp_median = IterativeImputer(max_iter=10, tol=0.001, n_nearest_features=10, initial_strategy='median',\n",
    "                                  skip_complete=False, verbose=2, add_indicator=False)\n",
    "    imp_mode = IterativeImputer(max_iter=10, tol=0.001, n_nearest_features=10, initial_strategy='most_frequent',\n",
    "                                skip_complete=False, verbose=2, add_indicator=False)\n",
    "    # MICE to impute missing float data\n",
    "    data_mice_float = df.select_dtypes(\"float\")\n",
    "    float_labels = data_mice_float.columns.tolist()\n",
    "    data_mice_float = imp_median.fit_transform(data_mice_float)\n",
    "    data_mice_float = pd.DataFrame(data=data_mice_float)\n",
    "    data_mice_float = data_mice_float.set_axis(float_labels, axis=1, inplace=False)\n",
    "    # MICE to impute missing categorical data\n",
    "    data_mice_cat = df.select_dtypes(exclude=\"float\")\n",
    "    cat_labels = data_mice_cat.columns.tolist()\n",
    "    data_mice_cat = imp_mode.fit_transform(data_mice_cat)\n",
    "    data_mice_cat = pd.DataFrame(data=data_mice_cat)\n",
    "    data_mice_cat = data_mice_cat.set_axis(cat_labels, axis=1, inplace=False)\n",
    "    data_mice = pd.concat([data_mice_cat, data_mice_float], axis=1)\n",
    "    # Order according to \"old\" column order\n",
    "    data_mice = data_mice.reindex(columns=labels)\n",
    "\n",
    "    print(\"Time taken: \" + str(time.time() - start) + \"\\n\\n\")\n",
    "\n",
    "    return data_mice\n",
    "\n",
    "\n",
    "def replace_nans_with_mode(df):\n",
    "    for column in df:\n",
    "        print(\"Replace \" + str(df[column].isnull().sum()) + \" nans for \" + column, end=\"\\n\")\n",
    "        if df[column].dtype == float:\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "        else:\n",
    "            df[column] = df[column].fillna(df[column].mode().iloc[0])\n",
    "\n",
    "    # df.fillna(df.mode().iloc[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "# loading dataset\n",
    "# df_raw = pd.read_excel('Data/application_data_small.xlsx')\n",
    "def load_data(file_name='Data/application_data/application_data.csv'):\n",
    "    print(\"\\t---Loading dataset...---\")\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "def standardize_continuous_cols(df):\n",
    "    print(\"\\t---Standardize continuous data...---\")\n",
    "    # standardize continuous data with Z-transform\n",
    "    for column in df:\n",
    "        if df[column].dtype == float:\n",
    "            # TODO: WAT GEBEURT HIER MET MISSING DATA?\n",
    "            mean_col = np.mean(df[column])\n",
    "            stddev_col = np.std(df[column])\n",
    "            # print(\"column \" + column + \" has mean: \" + str(mean_col) + \" and stdev \" + str(stddev_col))\n",
    "\n",
    "            df[column] = (df[column] - mean_col) / stddev_col\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_data()\n",
    "df = standardize_continuous_cols(df)\n",
    "\n",
    "# remove missing data\n",
    "print(\"\\t---Remove missing data...---\")\n",
    "df = remove_columns_missing_values(df)\n",
    "# df = remove_rows_with_missing_values(df)\n",
    "\n",
    "print(\"\\t---Transform data to numeric...---\")\n",
    "# Transform categorical string data to numeric\n",
    "# remove variables which are a constant\n",
    "for column in df:\n",
    "    if len(df[column].unique()) == 1:\n",
    "        print(\"Removing constant column: \", column)\n",
    "        df = df.drop(column, axis=1)\n",
    "\n",
    "\n",
    "df[\"FLAG_OWN_CAR\"] = df[\"FLAG_OWN_CAR\"].replace(['Y', 'N'], [1, 0])\n",
    "df[\"FLAG_OWN_REALTY\"] = df[\"FLAG_OWN_REALTY\"].replace(['Y', 'N'], [1, 0])\n",
    "df[\"EMERGENCYSTATE_MODE\"] = df[\"EMERGENCYSTATE_MODE\"].replace(['Yes', 'No'], [1, 0])\n",
    "\n",
    "for column in df:\n",
    "    if df[column].dtype == object:\n",
    "        # print(column)\n",
    "        # print(df[column].unique())\n",
    "        # print(df[column].value_counts(ascending=False))\n",
    "        # print(\"Number of missing values for \" + str(column) + \": \" + str(df.isnull().sum()[column]) + \"\\n\\n\")\n",
    "        df[column] = df[column].replace(df[column].unique().tolist(), [*range(1, len(df[column].unique()) + 1)])\n",
    "\n",
    "\n",
    "print(\"\\t---MICE...---\")\n",
    "# df = replace_nans_with_mode(df)\n",
    "df = apply_MICE(df)\n",
    "\n",
    "\n",
    "\n",
    "# code for one-hot encoding\n",
    "# # creating initial dataframe\n",
    "# bridge_types = ('Arch','Beam','Truss','Cantilever','Tied Arch','Suspension','Cable')\n",
    "# bridge_df = pd.DataFrame(bridge_types, columns=['Bridge_Types'])\n",
    "# # generate binary values using get_dummies\n",
    "# dum_df = pd.get_dummies(bridge_df, columns=[\"Bridge_Types\"], prefix=[\"Type_is\"] )\n",
    "# # merge with main df bridge_df on key values\n",
    "# bridge_df = bridge_df.join(dum_df)\n",
    "# bridge_df\n",
    "\n",
    "\n",
    "# print(df['CODE_GENDER'].value_counts(ascending=False))\n",
    "\n",
    "# setting predictors and targets\n",
    "target = df[\"TARGET\"]\n",
    "X = df.iloc[:, 2:]\n",
    "\n",
    "# perform feature selection with ANOVA\n",
    "# print(\"\\t---perform feature selection with ANOVA...---\")\n",
    "# num_features = 20\n",
    "\n",
    "# Create an SelectKBest object to select features with k best ANOVA F-Values\n",
    "# fvalue_selector = feature_selection.SelectKBest(feature_selection.f_classif, k=num_features)\n",
    "#\n",
    "# # Apply the SelectKBest object to the features and target\n",
    "# X_kbest = fvalue_selector.fit_transform(X, target)\n",
    "#\n",
    "# print(X_kbest)\n",
    "#\n",
    "# # pipeline = pipeline.make_pipeline(fvalue_selector)\n",
    "#\n",
    "# exit(1)\n",
    "\n",
    "# perform pearson correlation\n",
    "# correlations = df.corr(method=\"pearson\")\n",
    "# sns.set(style=\"whitegrid\", font_scale=1)\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# plt.title('Pearson Correlation Matrix', fontsize=25)\n",
    "# sns.heatmap(correlations, linewidths=0.25, square=True, cmap=\"GnBu\", linecolor='w',\n",
    "#             annot=False, cbar_kws={\"shrink\": .7})\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "print(\"\\t---Perform cross-validation...---\")\n",
    "# transform data to numpy arrays\n",
    "y = np.array(target)\n",
    "x = np.array(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement k-fold cross-validation\n",
    "k = 2\n",
    "kf = model_selection.KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "\n",
    "for train_i, test_i in kf.split(x):\n",
    "    x_train, x_test = x[train_i, :], x[test_i, :]\n",
    "    y_train, y_test = y[train_i], y[test_i]\n",
    "\n",
    "    # balance out data with SMOTE\n",
    "    print(\"\\t---perform SMOTE...---\")\n",
    "    smote = over_sampling.SMOTE(random_state=0)\n",
    "    x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "    # make a simple logistic regression model\n",
    "    print(\"\\t---perform simple logistic regression...---\")\n",
    "    modelLogReg = linear_model.LogisticRegression(solver=\"liblinear\", random_state=0).fit(x_smote, y_smote)\n",
    "    print(\"\\t---making decision tree...---\")\n",
    "    modelDecTree = tree.DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\").fit(x_smote, y_smote)\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    # print(metrics.confusion_matrix(y_test, modelLogReg.predict(x_test)))\n",
    "    # # calculate the confusion matrix\n",
    "    # print(metrics.confusion_matrix(y_test, modelDecTree.predict(x_test)))\n",
    "\n",
    "    print(metrics.classification_report(y_test, modelLogReg.predict(x_test)))\n",
    "    print(metrics.classification_report(y_test, modelDecTree.predict(x_test)))\n",
    "\n",
    "# Precision: How many retrieved items are relevant?\n",
    "# Recall: How many relevant items are retrieved?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea9dd9b0e7e7a2e4d2d6763ebe5de50feef02fc563f913726dd00c719c2dabff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('3.7.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
